@article{koBriefReviewFacial2018,
  title = {A {{Brief Review}} of {{Facial Emotion Recognition Based}} on {{Visual Information}}},
  author = {Ko, Byoung},
  date = {2018-01-30},
  journaltitle = {Sensors},
  volume = {18},
  number = {2},
  pages = {401},
  issn = {1424-8220},
  doi = {10.3390/s18020401},
  url = {https://www.mdpi.com/1424-8220/18/2/401},
  urldate = {2024-09-04},
  abstract = {Facial emotion recognition (FER) is an important topic in the fields of computer vision and artificial intelligence owing to its significant academic and commercial potential. Although FER can be conducted using multiple sensors, this review focuses on studies that exclusively use facial images, because visual expressions are one of the main information channels in interpersonal communication. This paper provides a brief review of researches in the field of FER conducted over the past decades. First, conventional FER approaches are described along with a summary of the representative categories of FER systems and their main algorithms. Deep-learning-based FER approaches using deep networks enabling “end-to-end” learning are then presented. This review also focuses on an up-to-date hybrid deep-learning approach combining a convolutional neural network (CNN) for the spatial features of an individual frame and long short-term memory (LSTM) for temporal features of consecutive frames. In the later part of this paper, a brief review of publicly available evaluation metrics is given, and a comparison with benchmark results, which are a standard for a quantitative comparison of FER researches, is described. This review can serve as a brief guidebook to newcomers in the field of FER, providing basic knowledge and a general understanding of the latest state-of-the-art studies, as well as to experienced researchers looking for productive directions for future work.},
  langid = {english},
  file = {files/12/Ko - 2018 - A Brief Review of Facial Emotion Recognition Based on Visual Information.pdf}
}

@article{sariyanidiAutomaticAnalysisFacial2015,
  title = {Automatic {{Analysis}} of {{Facial Affect}}: {{A Survey}} of {{Registration}}, {{Representation}}, and {{Recognition}}},
  shorttitle = {Automatic {{Analysis}} of {{Facial Affect}}},
  author = {Sariyanidi, Evangelos and Gunes, Hatice and Cavallaro, Andrea},
  date = {2015-06-01},
  journaltitle = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {37},
  number = {6},
  pages = {1113--1133},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2014.2366127},
  url = {http://ieeexplore.ieee.org/document/6940284/},
  urldate = {2024-09-04},
  abstract = {Automatic affect analysis has attracted great interest in various contexts including the recognition of action units and basic or non-basic emotions. In spite of major efforts, there are several open questions on what the important cues to interpret facial expressions are and how to encode them. In this paper, we review the progress across a range of affect recognition applications to shed light on these fundamental questions. We analyse the state-of-the-art solutions by decomposing their pipelines into fundamental components, namely face registration, representation, dimensionality reduction and recognition. We discuss the role of these components and highlight the models and new trends that are followed in their design. Moreover, we provide a comprehensive analysis of facial representations by uncovering their advantages and limitations; we elaborate on the type of information they encode and discuss how they deal with the key challenges of illumination variations, registration errors, head-pose variations, occlusions, and identity bias. This survey allows us to identify open issues and to define future directions for designing real-world affect recognition systems.},
  langid = {english},
  file = {files/10/Sariyanidi et al. - 2015 - Automatic Analysis of Facial Affect A Survey of Registration, Representation, and Recognition.pdf}
}
